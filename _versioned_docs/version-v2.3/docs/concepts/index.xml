<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title> â€“ Concepts</title>
    <link>https://docs.ondat.io/v2.3/docs/concepts/</link>
    <description>Recent content in Concepts on </description>
    <generator>Hugo -- gohugo.io</generator>
    
	  <atom:link href="https://docs.ondat.io/v2.3/docs/concepts/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>Docs: Architecture</title>
      <link>https://docs.ondat.io/v2.3/docs/concepts/architecture/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://docs.ondat.io/v2.3/docs/concepts/architecture/</guid>
      <description>
        
        
        &lt;p&gt;Ondat is a software-defined storage platform for running stateful
applications in Kubernetes.&lt;/p&gt;
&lt;p&gt;Fundamentally, Ondat uses the storage attached to the nodes in the
Ondat cluster to create and present virtual volumes into containers. Space
on the host is consumed from the mount point &lt;code&gt;/var/lib/storageos/data&lt;/code&gt;, so it
is therefore recommended that disk devices are used exclusively for Ondat,
as described in &lt;a href=&#34;https://docs.ondat.io/v2.3/docs/operations/managing-host-storage/&#34;&gt;Managing Host Storage &lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ondat is agnostic to the underlying storage and runs equally well on
bare metal, in virtual machines or on cloud providers.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://docs.ondat.io/v2.3/images/docs/concepts/storageos-cluster.png&#34; alt=&#34;Ondat architecture&#34;&gt;&lt;/p&gt;
&lt;p&gt;Read about &lt;a href=&#34;https://storageos.com/storageos-cloud-native-storage&#34;&gt;the cloud native storage principles behind
Ondat&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;ondat-on-kubernetes&#34;&gt;Ondat on Kubernetes&lt;/h3&gt;
&lt;p&gt;Ondat is architected as a series of containers that fulfil separate,
discrete functions.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Ondat Controlplane&lt;/p&gt;
&lt;p&gt;Responsible for monitoring and maintaining the state of volumes and nodes in the cluster&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ondat Dataplane&lt;/p&gt;
&lt;p&gt;Responsible for all I/O path related tasks; reading, writing, compression and caching&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ondat Scheduler&lt;/p&gt;
&lt;p&gt;Responsible for scheduling applications on the same node as applications&#39; volumes&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;CSI helper&lt;/p&gt;
&lt;p&gt;Responsible for registering Ondat with Kubernetes as a CSI driver&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ondat Operator&lt;/p&gt;
&lt;p&gt;Responsible for the creation and maintenance of the Ondat cluster&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ondat is deployed by the Ondat operator. In Kubernetes, the Ondat
Controlplane and Dataplane are deployed in a single pod managed by a daemonset.
This daemonset runs on every node in the cluster that will consume or present
storage. The scheduler, CSI helper and Operator run as separate pods and are
controlled as deployments.&lt;/p&gt;
&lt;p&gt;Ondat is designed to feel familiar to Kubernetes and Docker users. Storage
is managed through standard StorageClasses and PersistentVolumeClaims, and
&lt;a href=&#34;https://docs.ondat.io/v2.3/docs/reference/labels/&#34;&gt;features&lt;/a&gt; are controlled by
Kubernetes-style labels and selectors, prefixed with &lt;code&gt;storageos.com/&lt;/code&gt;. By
default, volumes are cached to improve read performance and compressed to
reduce network traffic.&lt;/p&gt;
&lt;p&gt;Any pod may mount a Ondat virtual volume from any node that is also
running Ondat, regardless of whether the pod and volume are
collocated on the same node. Therefore, applications may be started or
restarted on any node and access volumes transparently.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Clusters</title>
      <link>https://docs.ondat.io/v2.3/docs/concepts/clusters/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://docs.ondat.io/v2.3/docs/concepts/clusters/</guid>
      <description>
        
        
        &lt;p&gt;Ondat clusters represent groups of nodes which run a common distributed
control plane.&lt;/p&gt;
&lt;p&gt;Typically, a Ondat cluster maps one-to-one to a Kubernetes (or similar
orchestrator) cluster, and we expect our daemonset to run on all worker
nodes within the cluster that will consume or present storage.&lt;/p&gt;
&lt;p&gt;Clusters use etcd to maintain state and manage distributed consensus between
nodes.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Compression</title>
      <link>https://docs.ondat.io/v2.3/docs/concepts/compression/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://docs.ondat.io/v2.3/docs/concepts/compression/</guid>
      <description>
        
        
        &lt;p&gt;Ondat compression is handled on a per volume basis and is disabled by
default in v2.2+, as performance is generally increased when compression is
disabled due to block alignment. This means that there is a trade
off between volume performance and the space the volume occupies on the backend
device.&lt;/p&gt;
&lt;p&gt;Compression can be enabled by setting the &lt;a href=&#34;https://docs.ondat.io/v2.3/docs/reference/labels&#34;&gt;label&lt;/a&gt;
&lt;code&gt;storageos.com/nocompress=false&lt;/code&gt; on a volume at volume creation time.&lt;/p&gt;
&lt;p&gt;Ondat utilises the &lt;a href=&#34;https://lz4.github.io/lz4/&#34;&gt;lz4 compression algorithm&lt;/a&gt;
when writing to the backend store and when compressing &lt;a href=&#34;https://docs.ondat.io/v2.3/docs/concepts/replication&#34;&gt;replication
traffic&lt;/a&gt; before it is sent across the network.&lt;/p&gt;
&lt;p&gt;Ondat detects whether a block can be compressed or not by creating a
heuristic that predicts the size of a compressed block. If the heuristic
indicates that the compressed block is likely to be larger than the
original block then the uncompressed block is stored. Block size increases post
compression if the compression dictionary is added to a block that cannot be
compressed. By verifying whether blocks can be compressed, disk efficiency is
increased and CPU resources are not wasted on attempts to compress
uncompressible blocks. Ondat&#39; patented on disk format is used to tell
whether individual blocks are compressed without overhead. As such volume
compression can be dynamically enabled/disabled even while a volume is in use.&lt;/p&gt;
&lt;p&gt;When compression and &lt;a href=&#34;https://docs.ondat.io/v2.3/docs/concepts/encryption&#34;&gt;encryption&lt;/a&gt; are both enabled
for a volume, blocks are compressed then encrypted.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Etcd</title>
      <link>https://docs.ondat.io/v2.3/docs/concepts/etcd/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://docs.ondat.io/v2.3/docs/concepts/etcd/</guid>
      <description>
        
        
        &lt;p&gt;&lt;a href=&#34;https://etcd.io&#34;&gt;Etcd&lt;/a&gt; is an open-source distributed, strongly consistent key
value store that is used by Ondat to durably persist the Ondat cluster
state. As the backing store for Kubernetes, Ondat uses etcd for many of the
same reasons.&lt;/p&gt;
&lt;p&gt;Ondat uses etcd as the single source of truth for all Ondat objects.
Whenever a request is made to create, update or delete an object the result is
written to etcd before the request is completed. Using etcd as a configuration
store allows nodes to retrieve the current cluster state after being offlined,
allowing offlined nodes to rejoin the cluster.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;N.B. Ondat v2.0 does not provide an embedded etcd server as previous
versions did. You will need to setup an etcd server for Ondat to use
prior to installation of Ondat. Please see our &lt;a href=&#34;https://docs.ondat.io/v2.3/docs/prerequisites/etcd/&#34;&gt;etcd prerequisites&lt;/a&gt; page for more information on how
to install and configure etcd.&lt;/p&gt;
&lt;/blockquote&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Namespaces</title>
      <link>https://docs.ondat.io/v2.3/docs/concepts/namespaces/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://docs.ondat.io/v2.3/docs/concepts/namespaces/</guid>
      <description>
        
        
        &lt;p&gt;Ondat namespaces are an identical concept to Kubernetes namespaces. They
are intended to allow a Ondat cluster to be used by multiple teams across
multiple projects.&lt;/p&gt;
&lt;p&gt;It is not necessary to create Ondat namespaces manually, as Ondat maps
Kubernetes namespaces on a one-to-one basis when PersistentVolumeClaims using
the Ondat StorageClass are created.&lt;/p&gt;
&lt;p&gt;Access to Namespaces is controlled through user or group level &lt;a href=&#34;https://docs.ondat.io/v2.3/docs/concepts/policies/&#34;&gt;policies&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Nodes</title>
      <link>https://docs.ondat.io/v2.3/docs/concepts/nodes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://docs.ondat.io/v2.3/docs/concepts/nodes/</guid>
      <description>
        
        
        &lt;p&gt;A Ondat node is any machine (virtual or physical) that is running the
Ondat daemonset pod. A node must be running a daemonset pod in order to
consume and/or present storage.&lt;/p&gt;
&lt;p&gt;Nodes can be run in several modes.&lt;/p&gt;
&lt;h2 id=&#34;hyperconverged-mode&#34;&gt;Hyperconverged Mode&lt;/h2&gt;
&lt;p&gt;By default Ondat nodes run in &lt;code&gt;hyperconverged&lt;/code&gt; mode. This means that the
node hosts data from Ondat volumes and can present volumes to applications.&lt;/p&gt;
&lt;p&gt;A hyperconverged node can store data from a volume and present volumes to
applications regardless of whether the data for the volume consumed is placed
on that node or is being served remotely. Remote volumes like this are handled
by an internal protocol to present block device access to applications running
on different nodes from the one to which their backing data store is attached.&lt;/p&gt;
&lt;p&gt;Ondat implements an extension of a Kubernetes Scheduler object that
influences the placement of Pods on the same nodes as their data.&lt;/p&gt;
&lt;h2 id=&#34;compute-only-mode&#34;&gt;Compute-only Mode&lt;/h2&gt;
&lt;p&gt;Alternatively, a node can run in &lt;code&gt;computeonly&lt;/code&gt; mode, which means no storage is
consumed on the node itself and the node only presents volumes hosted by
other nodes. Volumes presented to applications running on compute only nodes
are therefore all remote. Compute only nodes can be very useful for topologies
where nodes are ephemeral and should not host data, but the ephemeral nodes
host applications that require Ondat volumes. The nodes that are not
intended to hold data, but just to present Ondat volumes, can be set as
&lt;code&gt;computeonly&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;A node can be marked as compute only at any point in time by adding the label
&lt;code&gt;storageos.com/computeonly=true&lt;/code&gt;, following the &lt;a href=&#34;https://docs.ondat.io/v2.3/docs/reference/labels/&#34;&gt;labels reference&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;storage-mode&#34;&gt;Storage Mode&lt;/h2&gt;
&lt;p&gt;Finally, nodes can be set to storage mode. Nodes set to storage mode don&amp;rsquo;t
present data locally - instead all data is accessed through the network. This
topology is enforced by tainting the relevant nodes to ensure that application
workloads cannot be scheduled there.&lt;/p&gt;
&lt;p&gt;This mode is ideal for ensuring maximum stability of data access as the node is
isolated from resource drains that may occur due to applications running
alongside. For redundancy purposes, in high load clusters it is ideal to have
several nodes running in this mode.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Policies</title>
      <link>https://docs.ondat.io/v2.3/docs/concepts/policies/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://docs.ondat.io/v2.3/docs/concepts/policies/</guid>
      <description>
        
        
        &lt;p&gt;Ondat policies are a way to control user and group access to Ondat
&lt;a href=&#34;https://docs.ondat.io/v2.3/docs/concepts/namespaces/&#34;&gt;Namespaces&lt;/a&gt;. To grant a user or group
access to a namespace, a policy needs to be created mapping the user or group
to the namespace.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note: Users always have access to the default namespace&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For more information on how to use policies, see the
&lt;a href=&#34;https://docs.ondat.io/v2.3/docs/operations/policies/&#34;&gt;Policies operations&lt;/a&gt; page.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: ReadWriteMany</title>
      <link>https://docs.ondat.io/v2.3/docs/concepts/rwx/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://docs.ondat.io/v2.3/docs/concepts/rwx/</guid>
      <description>
        
        
        &lt;blockquote&gt;
&lt;p&gt;Please note: Ondat Project edition is required to create RWX Volumes.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Ondat supports ReadWriteMany (RWX) &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes&#34;&gt;access
mode&lt;/a&gt;
Persistent Volumes. A RWX PVC can be used simultaneously by many Pods in the
same Kubernetes namespace for read and write operations.&lt;/p&gt;
&lt;p&gt;Ondat RWX Volumes are based on a shared filesystem - in the case of our
implementation, this is NFS.&lt;/p&gt;
&lt;h2 id=&#34;architecture&#34;&gt;Architecture&lt;/h2&gt;
&lt;p&gt;For each RWX Volume, the following components are involved:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ondat ReadWriteOnly (RWO) Volume&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Ondat provisions a standard &lt;a href=&#34;https://docs.ondat.io/v2.3/docs/concepts/volumes/&#34;&gt;Volume&lt;/a&gt; that provides a block device for the file system of the NFS server. This
means that every RWX Volume has its own RWO Volume. This allows RWX Volumes to
leverage the synchronous replication and automatic failover functionality of
Ondat, providing the NFS server with high availability.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NFS-Ganesha server&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For each RWX Volume, an NFS-Ganesha server is spawned by Ondat. The NFS
server runs in user space on the Node containing the primary Volume. Each NFS
server uses its own RWO Volume to store data so the data of each Volume is
isolated.&lt;/p&gt;
&lt;p&gt;Ondat binds an ephemeral port to the host network interface for each
NFS-Ganesha server. The NFS export is presented using NFS v4.2. Check the
&lt;a href=&#34;https://docs.ondat.io/v2.3/docs/prerequisites/firewalls/&#34;&gt;prerequisites page&lt;/a&gt; to see the
range of ports needed for Ondat RWX Volumes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ondat API Manager&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Ondat fully integrates with Kubernetes. The Ondat API Manager Pod
monitors Ondat RWX Volumes to create and maintain a Kubernetes Service
that points towards each RWX Volume&amp;rsquo;s NFS export endpoint. The API Manager is
responsible for updating the Service endpoint when a RWX Volume failover
occurs.&lt;/p&gt;
&lt;h2 id=&#34;provisioning-and-using-rwx-pvcs&#34;&gt;Provisioning and using RWX PVCs&lt;/h2&gt;
&lt;p&gt;The sequence in which a RWX PVC is provisioned and used is as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A PersistentVolumeClaim (PVC) is created with RWX access mode using any
Ondat StorageClass.&lt;/li&gt;
&lt;li&gt;Ondat dynamically provisions the PV.&lt;/li&gt;
&lt;li&gt;A new Ondat RWO Volume is provisioned internally (not visible in
Kubernetes).&lt;/li&gt;
&lt;li&gt;When the RWX PVC is consumed by a pod, an NFS-Ganesha server is instantiated
on the same Node as the primary Volume. The NFS-Ganesha server thus uses the
RWO Ondat Volume as its back end disk.&lt;/li&gt;
&lt;li&gt;The Ondat API Manager publishes the host IP and port for the NFS service
endpoint, by creating a Kubernetes Service that points to the NFS-Ganesha
server export endpoint.&lt;/li&gt;
&lt;li&gt;Ondat issues a NFS mount on the Node where the Pod using the PVC is
scheduled.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;high-availability&#34;&gt;High availability&lt;/h2&gt;
&lt;p&gt;RWX Volumes failover in the same way as standard RWO Ondat Volumes. The
replica Volume is promoted upon detection of Node failure and the NFS-Ganesha
server is started on the Node containing the promoted replica. The Ondat
API Manager updates the endpoint of the Volume&amp;rsquo;s NFS service, causing traffic
to be routed to the URL of the new NFS-Ganesha server. The NFS client in the
application Node (where the user&amp;rsquo;s Pod is running) automatically reconnects.&lt;/p&gt;
&lt;h2 id=&#34;notes&#34;&gt;Notes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;All feature labels that work on RWO Volumes will also work on RWX Volumes.&lt;/li&gt;
&lt;li&gt;A Ondat RWX Volume is matched one-to-one with a PVC. Therefore the
Ondat RWX Volume can only be accessed by Pods in the same Kubernetes
namespace.&lt;/li&gt;
&lt;li&gt;Ondat RWX Volumes support volume resize. Refer to the &lt;a href=&#34;https://docs.ondat.io/v2.3/docs/operations/resize/&#34;&gt;resize&lt;/a&gt; documentation for more details.&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Replication</title>
      <link>https://docs.ondat.io/v2.3/docs/concepts/replication/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://docs.ondat.io/v2.3/docs/concepts/replication/</guid>
      <description>
        
        
        &lt;p&gt;Ondat replicates volumes across nodes for data protection and high
availability. Synchronous replication ensures strong consistency for
applications such as databases and Elasticsearch, incurring one network round
trip on writes.&lt;/p&gt;
&lt;p&gt;The basic model for Ondat replication is of a master volume with distributed
replicas. Each volume can be replicated between 0 and 5 times, which are
provisioned to 0 to 5 nodes, up to the number of remaining nodes in the cluster.&lt;/p&gt;
&lt;p&gt;In this diagram, the master volume &lt;code&gt;D&lt;/code&gt; was created on node 1, and two replicas,
&lt;code&gt;D2&lt;/code&gt; and &lt;code&gt;D3&lt;/code&gt; on nodes 3 and 5.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://docs.ondat.io/v2.3/images/docs/concepts/high-availability.png&#34; alt=&#34;Ondat replication&#34;&gt;&lt;/p&gt;
&lt;p&gt;Writes that come into &lt;code&gt;D&lt;/code&gt; (step 1) are written in parallel to &lt;code&gt;D2&lt;/code&gt; and &lt;code&gt;D3&lt;/code&gt;
(step 2). When both replicas and the master acknowledge that the data has been
written (step 3), the write operation return successfully to the application
(step 4).&lt;/p&gt;
&lt;p&gt;For most applications, one replica is sufficient (&lt;code&gt;storageos.com/replicas=1&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;All replication traffic on the wire is compressed using the lz4 algorithm, then
streamed over tcp/ip to target port tcp/5703.&lt;/p&gt;
&lt;p&gt;If the master volume is lost, a replica is promoted to master (&lt;code&gt;D2&lt;/code&gt; or &lt;code&gt;D3&lt;/code&gt;
above) and a new replica is created and synced on an available node (Node 2 or
4). This is transparent to the application and does not cause downtime.&lt;/p&gt;
&lt;p&gt;If a replica volume is lost and there are enough remaining nodes, a new replica
is created and synced on an available node. While a new replica is created and
being synced, the volume&amp;rsquo;s health will be marked as degraded.&lt;/p&gt;
&lt;p&gt;If the lost replica comes back online before the new replica has finished
synchronizing, then Ondat will calculate which of the two synchronizing
replicas has the smallest difference compared to the master volume and keep
that replica. The same holds true if a master volume is lost and a replica is
promoted to be the new master. If possible, a new replica will be created and
begin to sync. Should the former master come back online it will be demoted to
a replica and the replica will the smallest difference to the current master
will be kept.&lt;/p&gt;
&lt;p&gt;While the replica count is controllable on a per-volume basis, some
environments may prefer to set &lt;a href=&#34;https://docs.ondat.io/v2.3/docs/reference/labels/#storageos-storageclass-labels&#34;&gt;default labels on the StorageClass&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;number-of-nodes&#34;&gt;Number of nodes&lt;/h3&gt;
&lt;p&gt;Ondat replicas are distributed across available nodes. When a node fails,
a new replica is provisioned and synced as described above. Failure to
provision a new replica will eventually cause the volume to become read-only.
Ondat will re-attempt creation of the replica for 90 seconds. After that
period, if the old replica is not available and a new replica cannot be
provisioned, Ondat cannot guarantee that the data is safe and stored on
multiple nodes as requested by the user. It will therefore force the volume
to be set to read-only.&lt;/p&gt;
&lt;p&gt;To ensure that a new replica can always be created, an additional node should
be available. To guarantee high availability, clusters using volumes with 1
replica must have at least 3 storage nodes. When using volumes with 2 replicas,
at least 4 storage nodes, 3 replicas, 5 nodes, etc.&lt;/p&gt;
&lt;p&gt;Minimum number of storage nodes = 1 (primary) + N (replicas) + 1&lt;/p&gt;
&lt;p&gt;For an application to be able to use the volume normally, the volume needs to
recover from a degraded state. This is achieved by having enough nodes on which
to place the data, and triggering a remount of the volumes - i.e. deleting the
Pod mounting the volume once it is in a healthy state.&lt;/p&gt;
&lt;h3 id=&#34;delta-sync&#34;&gt;Delta Sync&lt;/h3&gt;
&lt;p&gt;Ondat implements a delta sync between a volume master and its replicas.
This means that if a replica for a volume goes offline, that when the replica
comes back online only the regions with changed blocks need to be synchronized.
This optimization reduces the time it takes for replicas to catch up, improving
volume resilience. Additionally, it reduces network and IO bandwidth which can
reduce costs when running in public clouds.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Volumes</title>
      <link>https://docs.ondat.io/v2.3/docs/concepts/volumes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://docs.ondat.io/v2.3/docs/concepts/volumes/</guid>
      <description>
        
        
        &lt;p&gt;Ondat volumes are a logical construct which represent a writeable volume
and exhibit standard POSIX semantics. Ondat presents volumes as mounts into
containers via the Linux LIO subsystem.&lt;/p&gt;
&lt;p&gt;Conceptually, Ondat volumes have a frontend presentation, which is what
the application sees, and a backend presentation, which is the actual on-disk
format. Depending on the configuration, frontend and backend components may be
on the same or different hosts.&lt;/p&gt;
&lt;p&gt;Volumes are formatted using the linux standard ext4 filesystem by default.
Kubernetes users may change the default filesystem type to ext2, ext3, ext4,
or xfs by setting the fsType parameter in their StorageClass (see
&lt;a href=&#34;https://docs.ondat.io/v2.3/docs/reference/filesystems#persistent-volume-filesystems&#34;&gt;Supported
Filesystems&lt;/a&gt; for
more information). Different filesystems may be supported in the future.&lt;/p&gt;
&lt;p&gt;Ondat volumes are represented on disk in two parts. Actual volume data is
written to blob files in &lt;code&gt;/var/lib/storageos/data/dev[\d+]&lt;/code&gt;. Inside these
directories, each Ondat block device gets two blob files of the form
&lt;code&gt;vol.xxxxxx.y.blob&lt;/code&gt;, where x is the inode number for the device, and y is an
index between 0 and 1. We provide two blob files in order to ensure that
certain operations which require locking do not impede in-flight writes to the
volume.&lt;/p&gt;
&lt;p&gt;In systems which have multiple &lt;code&gt;/var/lib/storageos/data/dev[\d+]&lt;/code&gt; directories,
two blob files are created per block device. This allows us to load-balance
writes across multiple devices. In cases where dev directories are added after
a period of run time, later directories are favoured for writes until the data
is distributed evenly across the blob files.&lt;/p&gt;
&lt;p&gt;Metadata is kept in directories named &lt;code&gt;/var/lib/storageos/data/db[\d+]&lt;/code&gt;. We
maintain an index of all blocks written to the blob file inside the metadata
store, including checksums. These checksums allow us to detect bitrot, and
return errors on reads, rather than serve bad data. In future versions we may
implement recovery from replicas for volumes with one or more replicas defined.&lt;/p&gt;
&lt;p&gt;Ondat metadata requires approximately 2.7GB of storage per 1TiB of allocated
blocks in the associated volume. This size is consistent irrespective of data
compression defined on the volume.&lt;/p&gt;
&lt;p&gt;To ensure deterministic performance, individual Ondat volumes must fit on a single
node.&lt;/p&gt;
&lt;h2 id=&#34;minimum-volume-size&#34;&gt;Minimum Volume Size&lt;/h2&gt;
&lt;p&gt;The minimum volume size Ondat supports is 1 GiB.&lt;/p&gt;
&lt;h2 id=&#34;volume-resize&#34;&gt;Volume Resize&lt;/h2&gt;
&lt;p&gt;Ondat v2.1 supports offline resize of volumes. This means that a volume
cannot be resized while it is in use. Furthermore, in order for a resize
operation to take place the volume must not be attached to a node. This is to
ensure that the volume is not in use.&lt;/p&gt;
&lt;p&gt;This means that if a Kubernetes pod is currently consuming a volume that a
resize request has been issued for, the resize will not be actioned until the
pod is terminated and the volume is detached from the node. The Ondat
controlplane will then attach the volume to the node that holds the master
deployment and resize the underlying block device and then run resize2fs to
expand the filesystem.&lt;/p&gt;
&lt;p&gt;For a walk through of how to resize a volume please see the &lt;a href=&#34;https://docs.ondat.io/v2.3/docs/operations/resize&#34;&gt;Volume
Resize&lt;/a&gt; operations page.&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>
