<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title> â€“ Concepts</title>
    <link>https://docs.ondat.io/v2.4/docs/concepts/</link>
    <description>Recent content in Concepts on </description>
    <generator>Hugo -- gohugo.io</generator>
    
	  <atom:link href="https://docs.ondat.io/v2.4/docs/concepts/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>Docs: Cluster Topologies</title>
      <link>https://docs.ondat.io/v2.4/docs/concepts/cluster-topologies/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://docs.ondat.io/v2.4/docs/concepts/cluster-topologies/</guid>
      <description>
        
        
        &lt;p&gt;Ondat makes it possible for you to organise your cluster in multiple ways,
depending on your priorities and needs. The approaches below are idealised
representations of possible Ondat clusters and can be mixed, modified and
changed at execution time.&lt;/p&gt;
&lt;p&gt;Ondat performs file I/O over the network, which is how we ensure that your
data is always available throughout your cluster. This also affords the user
certain possibilities for organising your cluster, as we suggest below.&lt;/p&gt;
&lt;h2 id=&#34;hyperconverged-model&#34;&gt;Hyperconverged Model&lt;/h2&gt;
&lt;p&gt;In this topology all nodes can store data and present data. This gives maximum
flexibility to the Ondat and Kubernetes schedulers, and maximum choice for
pod placement. No matter how you deploy your workloads they will be able to
store and access data on every node.&lt;/p&gt;
&lt;p&gt;By default we place workloads locally where possible - this means that we
default to a hybrid of the hyperconverged/high-performance models, maximising
performance without extra effort.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://docs.ondat.io/v2.4/images/docs/concepts/hyperconverged.png&#34; alt=&#34;Hyperconverged Model&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;centralised-storage-model&#34;&gt;Centralised Storage Model&lt;/h2&gt;
&lt;p&gt;In this cluster topology volume data is placed on a particular subset of nodes,
while the remaining nodes in your cluster are set to &lt;code&gt;computeonly&lt;/code&gt;, allowing
access to data hosted on other nodes, while consuming no storage themselves.
This model can be advantageous if, for example, you want to take advantage of
the hardware characteristics of particular nodes. A centralised storage model
can also help avoid problems with naive resource allocation, since storage
nodes and compute workloads can be kept apart.&lt;/p&gt;
&lt;p&gt;This mode is also very suitable for elastic fleets with burstable workloads. A
fleet can be expanded with many new machines for computing, while maintaining a
central data store not impacted by rapid and repeated cluster scaling.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://docs.ondat.io/v2.4/images/docs/concepts/centralised.png&#34; alt=&#34;Centralised Model&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;high-performance-mode&#34;&gt;High Performance Mode&lt;/h2&gt;
&lt;p&gt;In this mode we co-locate pods with the volumes they are using in order to take
advantage of the performance gains from running on the same node, while
retaining the utility of orchestrators for managing app lifecycle.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://docs.ondat.io/v2.4/images/docs/concepts/high-performance.png&#34; alt=&#34;High Performance Model&#34;&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Clusters</title>
      <link>https://docs.ondat.io/v2.4/docs/concepts/clusters/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://docs.ondat.io/v2.4/docs/concepts/clusters/</guid>
      <description>
        
        
        &lt;p&gt;Ondat clusters represent groups of nodes which run a common distributed
control plane.&lt;/p&gt;
&lt;p&gt;Typically, an Ondat cluster maps one-to-one to a Kubernetes (or similar
orchestrator) cluster, and we expect our daemonset to run on all worker
nodes within the cluster that will consume or present storage.&lt;/p&gt;
&lt;p&gt;Clusters use etcd to maintain state and manage distributed consensus between
nodes.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Compression</title>
      <link>https://docs.ondat.io/v2.4/docs/concepts/compression/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://docs.ondat.io/v2.4/docs/concepts/compression/</guid>
      <description>
        
        
        &lt;p&gt;Ondat compression is handled on a per volume basis and is disabled by
default in v2.2+, as performance is generally increased when compression is
disabled due to block alignment. This means that there is a trade
off between volume performance and the space the volume occupies on the backend
device.&lt;/p&gt;
&lt;p&gt;Compression can be enabled by setting the &lt;a href=&#34;https://docs.ondat.io/v2.4/docs/reference/labels&#34;&gt;label&lt;/a&gt;
&lt;code&gt;storageos.com/nocompress=false&lt;/code&gt; on a volume at volume creation time.&lt;/p&gt;
&lt;p&gt;Ondat utilises the &lt;a href=&#34;https://lz4.github.io/lz4/&#34;&gt;lz4 compression algorithm&lt;/a&gt;
when writing to the backend store and when compressing &lt;a href=&#34;https://docs.ondat.io/v2.4/docs/concepts/replication&#34;&gt;replication
traffic&lt;/a&gt; before it is sent across the network.&lt;/p&gt;
&lt;p&gt;Ondat detects whether a block can be compressed or not by creating a
heuristic that predicts the size of a compressed block. If the heuristic
indicates that the compressed block is likely to be larger than the
original block then the uncompressed block is stored. Block size increases post
compression if the compression dictionary is added to a block that cannot be
compressed. By verifying whether blocks can be compressed, disk efficiency is
increased and CPU resources are not wasted on attempts to compress
uncompressible blocks. Ondat&amp;rsquo;s patented on-disk format is used to tell
whether individual blocks are compressed without overhead. As such volume
compression can be dynamically enabled/disabled even while a volume is in use.&lt;/p&gt;
&lt;p&gt;When compression and &lt;a href=&#34;https://docs.ondat.io/v2.4/docs/concepts/encryption&#34;&gt;encryption&lt;/a&gt; are both enabled
for a volume, blocks are compressed then encrypted.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Etcd</title>
      <link>https://docs.ondat.io/v2.4/docs/concepts/etcd/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://docs.ondat.io/v2.4/docs/concepts/etcd/</guid>
      <description>
        
        
        &lt;p&gt;&lt;a href=&#34;https://etcd.io&#34;&gt;Etcd&lt;/a&gt; is an open-source distributed, strongly consistent key
value store that is used by Ondat to durably persist the Ondat cluster
state. As the backing store for Kubernetes, Ondat uses etcd for many of the
same reasons.&lt;/p&gt;
&lt;p&gt;Ondat uses etcd as the single source of truth for all Ondat objects.
Whenever a request is made to create, update or delete an object the result is
written to etcd before the request is completed. Using etcd as a configuration
store allows nodes to retrieve the current cluster state after being offlined,
allowing offlined nodes to rejoin the cluster.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;N.B. Ondat v2.0 does not provide an embedded etcd server as previous
versions did. You will need to setup an etcd server for Ondat to use
prior to installation of Ondat. For more information on how
to install and configure etcd, see our &lt;a href=&#34;https://docs.ondat.io/v2.4/docs/prerequisites/etcd/&#34;&gt;etcd prerequisites&lt;/a&gt; page.&lt;/p&gt;
&lt;/blockquote&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Fencing</title>
      <link>https://docs.ondat.io/v2.4/docs/concepts/fencing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://docs.ondat.io/v2.4/docs/concepts/fencing/</guid>
      <description>
        
        
        &lt;h2 id=&#34;statefulset-behaviour&#34;&gt;StatefulSet behaviour&lt;/h2&gt;
&lt;p&gt;In order to understand what Ondat Fencing for Kubernetes is and when it is
needed, it is required to first understand the behaviour of StatefulSets.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/&#34;&gt;StatefulSets&lt;/a&gt;
are the de facto Kubernetes controller to use for stateful applications. The
StatefulSet controller offers guarantees around pod uniqueness, sticky
identities and the persistence of PVCs beyond the lifetime of their pods. As
such, StatefulSets have different characteristics and provide different
guarantees than Deployments.&lt;/p&gt;
&lt;p&gt;Deployments guarantee the amount of healthy replicas by reconciling towards the
deployment desired  state. Attempts to align the number of healthy pods with
the deployment&amp;rsquo;s desired state happen as fast as possible by aggressively
initializing and terminating pods. If one pod is terminating, another will be
automatically scheduled to start even if the first pod is not yet completely
terminated. Stateless applications benefit from this behaviour as one pod
executes the same work as any other in the deployment.&lt;/p&gt;
&lt;p&gt;StatefulSets, on the other hand, guarantee that every pod scheduled has a
unique identity, which is to say that only a single copy of a pod is running in
the cluster at any one time. Whenever scheduling decisions are made, the
StatefulSet controller ensures that only one copy of this pod is running at any
time. If a pod is deleted, a new pod will not be scheduled until the first pod
is fully terminated. This is an important guarantee as FileSystems need to be
unmounted before they can be remounted in a new pod. Any ReadWriteOnce PVC
defining a device requires this behaviour to ensure the consistency of the data
and thus the PVC.&lt;/p&gt;
&lt;p&gt;To protect data integrity, Kubernetes guarantees that there will never be more
than one instance of a StatefulSet Pod running at a time. It assumes that when
a node is determined to be offline it may still be running the workload but
partitioned from the network. Since Kubernetes is unable to
verify that the Pod has been stopped it errs on the side of caution and does
not allow a replacement to start on another node.&lt;/p&gt;
&lt;p&gt;Kubernetes does reschedule pods from some controllers when nodes become
unavailable. The default behaviour is that when a node becomes unavailable its
status becomes &amp;ldquo;Unknown&amp;rdquo; and after the &lt;code&gt;pod-eviction-timeout&lt;/code&gt; has passed pods
are scheduled for deletion. By default, the &lt;code&gt;pod-eviction-timeout&lt;/code&gt; is 300
seconds.&lt;/p&gt;
&lt;p&gt;For this reason, Kubernetes requires manual intervention to initiate timely
failover of a StatefulSet Pod. The Ondat Fencing Controller gives the
capability to enable fast failover of workloads when a node goes offline.&lt;/p&gt;
&lt;p&gt;For more information on the rationale behind the design of StatefulSets please
see the Kubernetes design proposal for &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/pod-safety.md&#34;&gt;Pod
Safety&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;ondat-fencing-controller&#34;&gt;Ondat Fencing Controller&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;The Ondat Fencing Controller is part of the Ondat API Manager which
is deployed in high availability when Ondat is installed.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;HA for StatefulSet applications can be achieved with the Ondat Fencing
feature&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Since Ondat is able to determine when a node is no longer able to access a
volume and has protections in place to ensure that a partitioned or formerly
partitioned node can not continue to write data, it can work with Kubernetes to
perform safe, fast failovers of Pods, including those running in StatefulSets.&lt;/p&gt;
&lt;p&gt;When Ondat detects that a node has gone offline or become partitioned, it
marks the node offline and performs volume failover operations.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://github.com/storageos/api-manager/tree/master/controllers/fencer&#34;&gt;Ondat Fencing
Controller&lt;/a&gt;
watches for these node failures and determines if there are any pods assigned
to the failed node with the label &lt;code&gt;storageos.com/fenced=true&lt;/code&gt;, and if the pods
have any PVCs backed by Ondat volumes.&lt;/p&gt;
&lt;p&gt;When a Pod has Ondat volumes and if they are all healthy, the Ondat
fencing controller deletes the Pod to allow it to be rescheduled on another
node. It also deletes the VolumeAtachments for the corresponding volumes so
that they can be immediately attached to the new node.&lt;/p&gt;
&lt;p&gt;No changes are made to Pods that have Ondat volumes that are unhealthy.
This is usually because a volume was configured to not have any replicas, and the
node with the single copy of the data is offline. In this case it is better to
wait for the node to recover.&lt;/p&gt;
&lt;p&gt;Fencing works with both dynamically provisioned PVCs and PVCs referencing
pre-provisioned volumes.&lt;/p&gt;
&lt;p&gt;The fencing feature is opt-in and Pods must have the
&lt;code&gt;storageos.com/fenced=true&lt;/code&gt; label set, and be using at least one Ondat
volume, to enable fast failover.&lt;/p&gt;
&lt;p&gt;For more information about how to enable pod fencing, see our &lt;a href=&#34;https://docs.ondat.io/v2.4/docs/operations/fencing/&#34;&gt;Fencing
Operations&lt;/a&gt; page.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Namespaces</title>
      <link>https://docs.ondat.io/v2.4/docs/concepts/namespaces/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://docs.ondat.io/v2.4/docs/concepts/namespaces/</guid>
      <description>
        
        
        &lt;p&gt;Ondat namespaces are an identical concept to Kubernetes namespaces. They
are intended to allow an Ondat cluster to be used by multiple teams across
multiple projects.&lt;/p&gt;
&lt;p&gt;It is not necessary to create Ondat namespaces manually, as Ondat maps
Kubernetes namespaces on a one-to-one basis when PersistentVolumeClaims using
the Ondat StorageClass are created.&lt;/p&gt;
&lt;p&gt;Access to Namespaces is controlled through user or group level &lt;a href=&#34;https://docs.ondat.io/v2.4/docs/concepts/policies/&#34;&gt;policies&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Nodes</title>
      <link>https://docs.ondat.io/v2.4/docs/concepts/nodes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://docs.ondat.io/v2.4/docs/concepts/nodes/</guid>
      <description>
        
        
        &lt;p&gt;An Ondat node is any machine (virtual or physical) that is running the
Ondat daemonset pod. A node must be running a daemonset pod in order to
consume and/or present storage.&lt;/p&gt;
&lt;p&gt;Nodes can be run in several modes.&lt;/p&gt;
&lt;h2 id=&#34;hyperconverged-mode&#34;&gt;Hyperconverged Mode&lt;/h2&gt;
&lt;p&gt;By default Ondat nodes run in &lt;code&gt;hyperconverged&lt;/code&gt; mode. This means that the
node hosts data from Ondat volumes and can present volumes to applications.&lt;/p&gt;
&lt;p&gt;A hyperconverged node can store data from a volume and present volumes to
applications regardless of whether the data for the volume consumed is placed
on that node or is being served remotely. Remote volumes like this are handled
by an internal protocol to present block device access to applications running
on different nodes from the one to which their backing data store is attached.&lt;/p&gt;
&lt;p&gt;Ondat implements an extension of a Kubernetes Scheduler object that
influences the placement of Pods on the same nodes as their data.&lt;/p&gt;
&lt;h2 id=&#34;compute-only-mode&#34;&gt;Compute-only Mode&lt;/h2&gt;
&lt;p&gt;Alternatively, a node can run in &lt;code&gt;computeonly&lt;/code&gt; mode, which means no storage is
consumed on the node itself and the node only presents volumes hosted by
other nodes. Volumes presented to applications running on compute only nodes
are therefore all remote. Compute only nodes can be very useful for topologies
where nodes are ephemeral and should not host data, but the ephemeral nodes
host applications that require Ondat volumes. The nodes that are not
intended to hold data, but just to present Ondat volumes, can be set as
&lt;code&gt;computeonly&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;A node can be marked as compute only at any point in time by adding the label
&lt;code&gt;storageos.com/computeonly=true&lt;/code&gt;, following the &lt;a href=&#34;https://docs.ondat.io/v2.4/docs/reference/labels/&#34;&gt;labels reference&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;storage-mode&#34;&gt;Storage Mode&lt;/h2&gt;
&lt;p&gt;Finally, nodes can be set to storage mode. Nodes set to storage mode don&amp;rsquo;t
present data locally - instead all data is accessed through the network. This
topology is enforced by tainting the relevant nodes to ensure that application
workloads cannot be scheduled there.&lt;/p&gt;
&lt;p&gt;This mode is ideal for ensuring maximum stability of data access as the node is
isolated from resource drains that may occur due to applications running
alongside. For redundancy purposes, in high load clusters it is ideal to have
several nodes running in this mode.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Ondat Components</title>
      <link>https://docs.ondat.io/v2.4/docs/concepts/components/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://docs.ondat.io/v2.4/docs/concepts/components/</guid>
      <description>
        
        
        &lt;p&gt;Ondat is a software-defined storage platform for running stateful
applications in Kubernetes.&lt;/p&gt;
&lt;p&gt;Fundamentally, Ondat uses the storage attached to the nodes in the
Ondat cluster to create and present virtual volumes into containers. Space
on the host is consumed from the mount point &lt;code&gt;/var/lib/storageos/data&lt;/code&gt;, so it
is therefore recommended that disk devices are used exclusively for Ondat,
as described in &lt;a href=&#34;https://docs.ondat.io/v2.4/docs/operations/managing-host-storage/&#34;&gt;Managing Host Storage &lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ondat is agnostic to the underlying storage and runs equally well on
bare metal, in virtual machines or on cloud providers.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://docs.ondat.io/v2.4/images/docs/concepts/storageos-cluster.png&#34; alt=&#34;Ondat cluster components&#34;&gt;&lt;/p&gt;
&lt;p&gt;Read about &lt;a href=&#34;https://storageos.com/storageos-cloud-native-storage&#34;&gt;the cloud native storage principles behind
Ondat&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;ondat-on-kubernetes&#34;&gt;Ondat on Kubernetes&lt;/h3&gt;
&lt;p&gt;Ondat is architected as a series of containers that fulfil separate,
discrete functions.&lt;/p&gt;
&lt;p&gt;Links where appropriate have been given to our open-source GitHub repo.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/storageos/cluster-operator&#34;&gt;Ondat Cluster Operator&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Responsible for the creation and maintenance of the Ondat cluster. This
operator is primarily responsible for ensuring that all the relevant
applications are running in your cluster.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ondat Controlplane&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Responsible for monitoring and maintaining the state of volumes and nodes
in the cluster. The Controlplane and the Dataplane run together in a single
container, managed by a daemonset. The Controlplane works with etcd to maintain
state consensus in your cluster.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ondat Dataplane&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Responsible for all I/O path related tasks; reading, writing, compression
and caching.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ondat Scheduler&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Responsible for scheduling applications on the same node as an application&amp;rsquo;s
volumes. Ondat uses a custom Kubernetes scheduler to handle pod placement,
ensuring that volumes are deployed on the same nodes as the relevant workloads
as often as possible.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/storageos/external-provisioner&#34;&gt;CSI helper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Responsible for registering Ondat with Kubernetes as a CSI driver. It
is necessary because the internal persistent volume controller running in
Kubernetes controller-manager does not have any direct interfaces to CSI
drivers. It monitors PVC objects created by users and creates/deletes volumes
for them.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/storageos/api-manager&#34;&gt;Ondat API manager&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Acts as a middle-man between various APIs. It has all the capabilities of a
Kubernetes Operator and is also able to communicate with the Ondat control
plane API. This application handles typical operator tasks like labelling or
removing nodes from Ondat when removed from the Kubernetes. It is
continually  monitoring the state of the cluster and moving it towards the
desired state when necessary.&lt;/p&gt;
&lt;p&gt;Ondat is deployed by the Ondat Cluster Operator. In Kubernetes, the
Ondat Controlplane and Dataplane are deployed in a single pod managed by a
daemonset.  This daemonset runs on every node in the cluster that will consume
or present storage. The Scheduler, CSI helper, Cluster Operator and API Manager
run as separate pods and are controlled as deployments.&lt;/p&gt;
&lt;p&gt;Ondat is designed to feel familiar to Kubernetes and Docker users. Storage
is managed through standard StorageClasses and PersistentVolumeClaims, and
&lt;a href=&#34;https://docs.ondat.io/v2.4/docs/reference/labels/&#34;&gt;features&lt;/a&gt; are controlled by
Kubernetes-style labels and selectors, prefixed with &lt;code&gt;storageos.com/&lt;/code&gt;. By
default, volumes are cached to improve read performance and compressed to
reduce network traffic.&lt;/p&gt;
&lt;p&gt;Any pod may mount a Ondat virtual volume from any node that is also
running Ondat, regardless of whether the pod and volume are
collocated on the same node. Therefore, applications may be started or
restarted on any node and access volumes transparently.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Policies</title>
      <link>https://docs.ondat.io/v2.4/docs/concepts/policies/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://docs.ondat.io/v2.4/docs/concepts/policies/</guid>
      <description>
        
        
        &lt;p&gt;Ondat policies are a way to control user and group access to Ondat
&lt;a href=&#34;https://docs.ondat.io/v2.4/docs/concepts/namespaces/&#34;&gt;Namespaces&lt;/a&gt;. To grant a user or group
access to a namespace, a policy needs to be created mapping the user or group
to the namespace.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note: Users always have access to the default namespace&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For more information on how to use policies, see the
&lt;a href=&#34;https://docs.ondat.io/v2.4/docs/operations/policies/&#34;&gt;Policies operations&lt;/a&gt; page.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: ReadWriteMany</title>
      <link>https://docs.ondat.io/v2.4/docs/concepts/rwx/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://docs.ondat.io/v2.4/docs/concepts/rwx/</guid>
      <description>
        
        
        &lt;blockquote&gt;
&lt;p&gt;Please note: Ondat Project edition is required to create RWX Volumes.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Ondat supports ReadWriteMany (RWX) &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes&#34;&gt;access
mode&lt;/a&gt;
Persistent Volumes. A RWX PVC can be used simultaneously by many Pods in the
same Kubernetes namespace for read and write operations.&lt;/p&gt;
&lt;p&gt;Ondat RWX Volumes are based on a shared filesystem - in the case of our
implementation, this is NFS.&lt;/p&gt;
&lt;h2 id=&#34;architecture&#34;&gt;Architecture&lt;/h2&gt;
&lt;p&gt;For each RWX Volume, the following components are involved:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ondat ReadWriteOnly (RWO) Volume&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Ondat provisions a standard &lt;a href=&#34;https://docs.ondat.io/v2.4/docs/concepts/volumes/&#34;&gt;Volume&lt;/a&gt; that provides a block device for the file system of the NFS server. This
means that every RWX Volume has its own RWO Volume. This allows RWX Volumes to
leverage the synchronous replication and automatic failover functionality of
Ondat, providing the NFS server with high availability.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NFS-Ganesha server&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For each RWX Volume, an NFS-Ganesha server is spawned by Ondat. The NFS
server runs in user space on the Node containing the primary Volume. Each NFS
server uses its own RWO Volume to store data so the data of each Volume is
isolated.&lt;/p&gt;
&lt;p&gt;Ondat binds an ephemeral port to the host network interface for each
NFS-Ganesha server. The NFS export is presented using NFS v4.2. Check the
&lt;a href=&#34;https://docs.ondat.io/v2.4/docs/prerequisites/firewalls/&#34;&gt;prerequisites page&lt;/a&gt; to see the
range of ports needed for Ondat RWX Volumes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ondat API Manager&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Ondat fully integrates with Kubernetes. The Ondat API Manager Pod
monitors Ondat RWX Volumes to create and maintain a Kubernetes Service
that points towards each RWX Volume&amp;rsquo;s NFS export endpoint. The API Manager is
responsible for updating the Service endpoint when a RWX Volume failover
occurs.&lt;/p&gt;
&lt;h2 id=&#34;provisioning-and-using-rwx-pvcs&#34;&gt;Provisioning and using RWX PVCs&lt;/h2&gt;
&lt;p&gt;The sequence in which a RWX PVC is provisioned and used is as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A PersistentVolumeClaim (PVC) is created with RWX access mode using any
Ondat StorageClass.&lt;/li&gt;
&lt;li&gt;Ondat dynamically provisions the PV.&lt;/li&gt;
&lt;li&gt;A new Ondat RWO Volume is provisioned internally (not visible in
Kubernetes).&lt;/li&gt;
&lt;li&gt;When the RWX PVC is consumed by a pod, an NFS-Ganesha server is instantiated
on the same Node as the primary Volume. The NFS-Ganesha server thus uses the
RWO Ondat Volume as its back end disk.&lt;/li&gt;
&lt;li&gt;The Ondat API Manager publishes the host IP and port for the NFS service
endpoint, by creating a Kubernetes Service that points to the NFS-Ganesha
server export endpoint.&lt;/li&gt;
&lt;li&gt;Ondat issues a NFS mount on the Node where the Pod using the PVC is
scheduled.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;high-availability&#34;&gt;High availability&lt;/h2&gt;
&lt;p&gt;RWX Volumes failover in the same way as standard RWO Ondat Volumes. The
replica Volume is promoted upon detection of Node failure and the NFS-Ganesha
server is started on the Node containing the promoted replica. The Ondat
API Manager updates the endpoint of the Volume&amp;rsquo;s NFS service, causing traffic
to be routed to the URL of the new NFS-Ganesha server. The NFS client in the
application Node (where the user&amp;rsquo;s Pod is running) automatically reconnects.&lt;/p&gt;
&lt;h2 id=&#34;notes&#34;&gt;Notes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;All feature labels that work on RWO Volumes will also work on RWX Volumes.&lt;/li&gt;
&lt;li&gt;A Ondat RWX Volume is matched one-to-one with a PVC. Therefore the
Ondat RWX Volume can only be accessed by Pods in the same Kubernetes
namespace.&lt;/li&gt;
&lt;li&gt;Ondat RWX Volumes support volume resize. Refer to the &lt;a href=&#34;https://docs.ondat.io/v2.4/docs/operations/resize/&#34;&gt;resize&lt;/a&gt; documentation for more details.&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Replication</title>
      <link>https://docs.ondat.io/v2.4/docs/concepts/replication/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://docs.ondat.io/v2.4/docs/concepts/replication/</guid>
      <description>
        
        
        &lt;p&gt;Ondat replicates volumes across nodes for data protection and high
availability. Synchronous replication ensures strong consistency for
applications such as databases and Elasticsearch, incurring one network round
trip on writes.&lt;/p&gt;
&lt;p&gt;The basic model for Ondat replication is of a master volume with distributed
replicas. Each volume can be replicated between 0 and 5 times, which are
provisioned to 0 to 5 nodes, up to the number of remaining nodes in the cluster.&lt;/p&gt;
&lt;p&gt;In this diagram, the master volume &lt;code&gt;D&lt;/code&gt; was created on node 1, and two replicas,
&lt;code&gt;D2&lt;/code&gt; and &lt;code&gt;D3&lt;/code&gt; on nodes 3 and 5.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://docs.ondat.io/v2.4/images/docs/concepts/high-availability.png&#34; alt=&#34;Ondat replication&#34;&gt;&lt;/p&gt;
&lt;p&gt;Writes that come into &lt;code&gt;D&lt;/code&gt; (step 1) are written in parallel to &lt;code&gt;D2&lt;/code&gt; and &lt;code&gt;D3&lt;/code&gt;
(step 2). When both replicas and the master acknowledge that the data has been
written (step 3), the write operation return successfully to the application
(step 4).&lt;/p&gt;
&lt;p&gt;For most applications, one replica is sufficient (&lt;code&gt;storageos.com/replicas=1&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;All replication traffic on the wire is compressed using the lz4 algorithm, then
streamed over tcp/ip to target port tcp/5703.&lt;/p&gt;
&lt;p&gt;If the master volume is lost, a replica is promoted to master (&lt;code&gt;D2&lt;/code&gt; or &lt;code&gt;D3&lt;/code&gt;
above) and a new replica is created and synced on an available node (Node 2 or
4). This is transparent to the application and does not cause downtime.&lt;/p&gt;
&lt;p&gt;If a replica volume is lost and there are enough remaining nodes, a new replica
is created and synced on an available node. While a new replica is created and
being synced, the volume&amp;rsquo;s health will be marked as degraded.&lt;/p&gt;
&lt;p&gt;If the lost replica comes back online before the new replica has finished
synchronizing, then Ondat will calculate which of the two synchronizing
replicas has the smallest difference compared to the master volume and keep
that replica. The same holds true if a master volume is lost and a replica is
promoted to be the new master. If possible, a new replica will be created and
begin to sync. Should the former master come back online it will be demoted to
a replica and the replica will the smallest difference to the current master
will be kept.&lt;/p&gt;
&lt;p&gt;While the replica count is controllable on a per-volume basis, some
environments may prefer to set &lt;a href=&#34;https://docs.ondat.io/v2.4/docs/reference/labels/#storageos-storageclass-labels&#34;&gt;default labels on the StorageClass&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;delta-sync&#34;&gt;Delta Sync&lt;/h2&gt;
&lt;p&gt;Ondat implements a delta sync between a volume master and its replicas.
This means that if a replica for a volume goes offline, that when the replica
comes back online only the regions with changed blocks need to be synchronized.
This optimization reduces the time it takes for replicas to catch up, improving
volume resilience. Additionally, it reduces network and IO bandwidth which can
reduce costs when running in public clouds.&lt;/p&gt;
&lt;h2 id=&#34;failure-modes&#34;&gt;Failure Modes&lt;/h2&gt;
&lt;p&gt;Ondat failure modes offer different guarantees with regards to a volume&amp;rsquo;s
mode of operation in the face of replica failure. If the failure mode is not
specified it defaults to &lt;code&gt;Hard&lt;/code&gt;. Volume failure modes can be dynamically
updated at run time.&lt;/p&gt;
&lt;h3 id=&#34;hard&#34;&gt;Hard&lt;/h3&gt;
&lt;p&gt;Hard failure mode requires that the number of declared replicas matches the
available number of replicas at all times. If a replica fails Ondat will
attempt creation of a new replica for 90 seconds. After 90s if the old replica
is not available and a new replica cannot be provisioned, Ondat cannot
guarantee that the data is stored on the number of multiple nodes requested by
the user. Ondat will therefore set the volume to be read-only.&lt;/p&gt;
&lt;p&gt;If a volume has gone read-only there are two stages to making it read-write
again. Firstly, sufficient replicas must be provisioned to match the desired
replica count. Depending on your environment, additional nodes and/or disk
capacity may be required for this. Secondly, the volume must be remounted -
necessitating pod deletion/recreation in Kubernetes.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;storageos.com/failure-mode: hard
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Number of nodes required for hard failure mode&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When a node fails, a new replica is provisioned and synced as described above.
To ensure that a new replica can always be created, an additional node should
be available. To guarantee high availability using &lt;code&gt;storageos.com/failure-mode: hard&lt;/code&gt;, clusters using volumes with 1 replica must have at least 3 storage
nodes. When using volumes with 2 replicas, at least 4 storage nodes, 3
replicas, 5 nodes, etc.&lt;/p&gt;
&lt;p&gt;Minimum number of storage nodes = 1 (primary) + N (replicas) + 1&lt;/p&gt;
&lt;h3 id=&#34;soft&#34;&gt;Soft&lt;/h3&gt;
&lt;p&gt;Soft failure mode allows a volume to continue serving I/O even when a replica
goes offline and a new replica fails to provision. So long as there are not
less than max(1,  n-1) available replicas where n is the number of replicas for
the volume.&lt;/p&gt;
&lt;p&gt;For example, if a volume with 2 replicas loses 1 replica, then I/O would
continue to be served since 1 replica remaining &amp;gt;= max(1, 1). If a volume with
1 replica loses 1 replica, then I/O would halt after 90 seconds since 0
replicas remaining &amp;lt; max(1, 0).&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;storageos.com/failure-mode: soft
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Number of nodes required for soft failure mode&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To ensure that a &lt;code&gt;storageos.com/failure-mode: soft &lt;/code&gt; volume is highly available, clusters using volumes with 1 replica must have at
least 2 storage nodes. When using volumes with 2 replicas, at least 3 storage
nodes, 3 replicas, 3 nodes, etc.&lt;/p&gt;
&lt;p&gt;Minimum number of storage nodes = 1 (primary) + N (replicas)&lt;/p&gt;
&lt;h3 id=&#34;threshold&#34;&gt;Threshold&lt;/h3&gt;
&lt;p&gt;Threshold failure mode allows the user to set the minimum required number of
online replicas for a volume. For example for a volume with 2 replicas, setting
the threshold to 1 would allow a single replica to be offline, whereas setting
threshold to 0 would allow 2 replicas to be offline.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;storageos.com/failure-mode: &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;(&lt;/span&gt;0-5&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Number of nodes required for threshold failure mode&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The minimum number of nodes for a &lt;code&gt;threshold&lt;/code&gt; volume is determined by the
threshold that is set.&lt;/p&gt;
&lt;p&gt;Minimum number of storage nodes = 1 (primary) + T (threshold)&lt;/p&gt;
&lt;h3 id=&#34;alwayson&#34;&gt;AlwaysOn&lt;/h3&gt;
&lt;p&gt;AlwaysOn failure mode allows all replicas for a volume to be offline and keeps
the volume writeable. A volume with failure mode AlwaysOn will continue to
serve I/O regardless of how many replicas it currently has. This mode should be
used with caution as it effectively allows for only a single copy of the data
to be available.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;storageos.com/failure-mode: alwayson
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Number of nodes required for AlwaysOn failure mode&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A &lt;code&gt;storageos.com/failure-mode: alwayson&lt;/code&gt; volume is highly available albeit at
the cost of reliability. The minimum node count here is 1 as the loss of all
replicas will be tolerated.&lt;/p&gt;
&lt;p&gt;Minimum number of storage nodes = 1 (primary)&lt;/p&gt;
&lt;p&gt;For details about how to use the labels on the VolumesCheck, see the &lt;a href=&#34;https://docs.ondat.io/v2.4/docs/operations/failure-modes/&#34;&gt;failure modes operations&lt;/a&gt; page.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Volumes</title>
      <link>https://docs.ondat.io/v2.4/docs/concepts/volumes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://docs.ondat.io/v2.4/docs/concepts/volumes/</guid>
      <description>
        
        
        &lt;p&gt;Ondat volumes are a logical construct which represent a writeable volume
and exhibit standard POSIX semantics. Ondat presents volumes as mounts into
containers via the Linux LIO subsystem.&lt;/p&gt;
&lt;p&gt;Conceptually, Ondat volumes have a frontend presentation, which is what
the application sees, and a backend presentation, which is the actual on-disk
format. Depending on the configuration, frontend and backend components may be
on the same or different hosts.&lt;/p&gt;
&lt;p&gt;Volumes are formatted using the linux standard ext4 filesystem by default.
Kubernetes users may change the default filesystem type to ext2, ext3, ext4, or
xfs by setting the fsType parameter in their StorageClass (see &lt;a href=&#34;https://docs.ondat.io/v2.4/docs/reference/filesystems/#persistent-volume-filesystems&#34;&gt;Supported
Filesystems&lt;/a&gt; for more
information). Different filesystems may be supported in the future.&lt;/p&gt;
&lt;p&gt;Ondat volumes are represented on disk in two parts. Actual volume data is
written to blob files in &lt;code&gt;/var/lib/storageos/data/dev[\d+]&lt;/code&gt;. Inside these
directories, each Ondat block device gets two blob files of the form
&lt;code&gt;vol.xxxxxx.y.blob&lt;/code&gt;, where x is the inode number for the device, and y is an
index between 0 and 1. We provide two blob files in order to ensure that
certain operations which require locking do not impede in-flight writes to the
volume.&lt;/p&gt;
&lt;p&gt;In systems which have multiple &lt;code&gt;/var/lib/storageos/data/dev[\d+]&lt;/code&gt; directories,
two blob files are created per block device. This allows us to load-balance
writes across multiple devices. In cases where dev directories are added after
a period of run time, later directories are favoured for writes until the data
is distributed evenly across the blob files.&lt;/p&gt;
&lt;p&gt;Metadata is kept in directories named &lt;code&gt;/var/lib/storageos/data/db[\d+]&lt;/code&gt;. We
maintain an index of all blocks written to the blob file inside the metadata
store, including checksums. These checksums allow us to detect bitrot, and
return errors on reads, rather than serve bad data. In future versions we may
implement recovery from replicas for volumes with one or more replicas defined.&lt;/p&gt;
&lt;p&gt;Ondat metadata requires approximately 2.7GB of storage per 1TiB of allocated
blocks in the associated volume. This size is consistent irrespective of data
compression defined on the volume.&lt;/p&gt;
&lt;p&gt;To ensure deterministic performance, individual Ondat volumes must fit on a single
node.&lt;/p&gt;
&lt;h2 id=&#34;minimum-volume-size&#34;&gt;Minimum Volume Size&lt;/h2&gt;
&lt;p&gt;The minimum volume size Ondat supports is 1GB.&lt;/p&gt;
&lt;h2 id=&#34;trim&#34;&gt;TRIM&lt;/h2&gt;
&lt;p&gt;Ondat volumes support TRIM/Unmap which allows the space allocated to
deleted blocks to be reclaimed from the backend blob files that back each
volume when a TRIM call is made. Support for TRIM is enabled by default for all
uncompressed volumes, volumes are created without compression enabled by
default. For more information on how to TRIM a filesystem, see &lt;a href=&#34;https://docs.ondat.io/v2.4/docs/operations/trim/&#34;&gt;TRIM
operations&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;volume-resize&#34;&gt;Volume Resize&lt;/h2&gt;
&lt;p&gt;Ondat v2.1 supports offline resize of volumes. This means that a volume
cannot be resized while it is in use. Furthermore, in order for a resize
operation to take place the volume must not be attached to a node. This is to
ensure that the volume is not in use.&lt;/p&gt;
&lt;p&gt;This means that if a Kubernetes pod is currently consuming a volume that a
resize request has been issued for, the resize will not be actioned until the
pod is terminated and the volume is detached from the node. The Ondat
controlplane will then attach the volume to the node that holds the master
deployment and resize the underlying block device and then run resize2fs to
expand the filesystem.&lt;/p&gt;
&lt;p&gt;For a walk through of how to resize a volume please see the &lt;a href=&#34;https://docs.ondat.io/v2.4/docs/operations/resize/&#34;&gt;Volume
Resize&lt;/a&gt; operations page.&lt;/p&gt;
&lt;h2 id=&#34;volume-encryption&#34;&gt;Volume Encryption&lt;/h2&gt;
&lt;p&gt;Volumes can be configured on creation to have encryption-at-rest. Data
is encrypted with XTS-AES and decrypted upon use. Please see
the &lt;a href=&#34;https://docs.ondat.io/v2.4/docs/reference/encryption/&#34;&gt;Encryption&lt;/a&gt; reference page for
more information.&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>
